import re
import json
import os
import sys
import io
import uuid
import time
import requests
from pprint import pprint
import google.generativeai as genai
from datetime import datetime

# âœ… Set UTF-8 encoding for standard output
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
# âœ… Configure paths
INSERT_URL = "http://127.0.0.1:8000/api/insert_articles/"
HEADERS = {"Content-Type": "application/json"}

# âœ… Base directories
CONFIG_FILE = "./config.json"
DJANGO_API_URL = "http://127.0.0.1:8000/api/insert_articles/"

# âœ… Load API key
def load_api_key(config_file):
    try:
        with open(config_file, 'r', encoding='utf-8') as file:
            config = json.load(file)
            return config.get("api_key")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âŒ Error reading config file: {e}")
        return None

API_KEY = load_api_key(CONFIG_FILE)
if API_KEY is None:
    sys.exit("âŒ API key is missing. Please check your config file.")

# âœ… Configure AI API
genai.configure(api_key=API_KEY)

# âœ… Find `MatchedNewsData-*` directories
def find_matched_news_dirs(base_dir):
    return [
        os.path.join(base_dir, d)
        for d in os.listdir(base_dir)
        if os.path.isdir(os.path.join(base_dir, d)) and d.startswith("MatchedNewsData")
    ]

# âœ… Read JSON files
def read_json_files(directory):
    articles = []
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                json_data = json.load(file)
                articles.append(json_data)
    return articles

# âœ… AI Article Processing with Categorization
def process_articles_with_ai(articles):
    article_strings = [json.dumps(article, ensure_ascii=False, indent=4) for article in articles]
    articles_combined = "\n".join(article_strings)

    # âœ… AI Prompts for Summarization & Categorization
    title_prompt = "Bu haber makalelerine dayanarak, 3-4 kelimelik nesnel bir baÅŸlÄ±k oluÅŸturun:"
    short_summary_prompt = "Bu haber makalelerine dayanarak, 10-50 karakter arasÄ±nda kÄ±sa, betimleme iÃ§ermeyen, salt bilgi iÃ§eren bir Ã¶zet oluÅŸturun:"
    detailed_summary_prompt = "Bu haber makalelerine dayanarak, detaylÄ± ve salt bilgi iÃ§eren bir Ã¶zet oluÅŸturun:"
    category_prompt = """
    Bu haber makalesini aÅŸaÄŸÄ±daki kategorilerden birine atayÄ±n:
    Siyaset
    EÄŸlence
    Spor
    Teknoloji
    SaÄŸlÄ±k
    Ã‡evre
    Bilim
    EÄŸitim
    Ekonomi
    Seyahat
    Moda
    KÃ¼ltÃ¼r
    SuÃ§
    Yemek
    YaÅŸam TarzÄ±
    Ä°ÅŸ DÃ¼nyasÄ±
    DÃ¼nya Haberleri
    Oyun
    Otomotiv
    Sanat
    Tarih
    Uzay
    Ä°liÅŸkiler
    Din
    Ruh SaÄŸlÄ±ÄŸÄ±
    Magazin

    EÄŸer uygun bir kategori bulamazsan, 'Genel' olarak belirle. Bu kategorilerden baÅŸka hiÃ§bir ÅŸey yazma.
    """

    # âœ… Initialize AI model
    model = genai.GenerativeModel('gemini-1.5-flash')

    time.sleep(5)  
    try:
        title_response = model.generate_content(f"{title_prompt}\n{articles_combined}")
        title = title_response.text.strip()
    except Exception as e:
        print(f"âŒ Error generating title: {e}")
        title = "Title generation failed"

    time.sleep(5)
    try:
        short_summary_response = model.generate_content(f"{short_summary_prompt}\n{articles_combined}")
        short_summary = short_summary_response.text.strip()
    except Exception as e:
        print(f"âŒ Error generating short summary: {e}")
        short_summary = "Short summary failed"

    time.sleep(5)
    try:
        detailed_summary_response = model.generate_content(f"{detailed_summary_prompt}\n{articles_combined}")
        detailed_summary = detailed_summary_response.text.strip()
    except Exception as e:
        print(f"âŒ Error generating detailed summary: {e}")
        detailed_summary = "Detailed summary failed"

    time.sleep(5)
    try:
        category_response = model.generate_content(f"{category_prompt}\n{articles_combined}")
        category = category_response.text.strip()
    except Exception as e:
        print(f"âŒ Error generating category: {e}")
        category = "Genel"  # Default to "General"

    # âœ… Generate Unique ID for the article
    article_id = str(uuid.uuid4())

    # âœ… Extract sources as a list
    sources = list(set([article.get("source", "Unknown") for article in articles if "source" in article]))

    # âœ… Create structured output with Category
    output = {
        "id": None,
        "articleId": article_id,
        "title": title,
        "content": "",
        "summary": short_summary,
        "longerSummary": detailed_summary,
        "category": category,  # âœ… Assign AI-generated category
        "tags": [],
        "source": sources,
        "location": None,
        "popularityScore": 0,
        "createdAt": None,
        "image": None,
        "priority": None
    }
    return output


# âœ… Save JSON output
def save_json_output(data):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"objectified_summary_{timestamp}.json"
    output_filepath = os.path.join(OUTPUT_DIR, output_filename)

    with open(output_filepath, 'w', encoding='utf-8') as output_file:
        json.dump(data, output_file, ensure_ascii=False, indent=4)

    print(f"âœ… Output saved locally at {output_filepath}")
    return output_filepath

# âœ… Function to send JSON files one by one
def send_json_files():
    if not os.path.exists(JSON_DIR):
        print(f"âŒ Directory not found: {JSON_DIR}")
        return
    
    json_files = [f for f in os.listdir(JSON_DIR) if f.endswith(".json")]
    
    if not json_files:
        print("âŒ No JSON files found in objectified_jsons directory!")
        return

    print(f"ğŸ” Found {len(json_files)} objectified JSON files. Sending them to the backend...")

    for json_file in json_files:
        json_path = os.path.join(JSON_DIR, json_file)

        try:
            # âœ… Load JSON data
            with open(json_path, 'r', encoding='utf-8') as file:
                data = json.load(file)

            # âœ… Debugging - Print data before sending
            print(f"\nğŸ”¹ Sending file: {json_file} to backend...")
            print("ğŸ“œ JSON Data (First 300 characters):", json.dumps(data, indent=2)[:300])

            # âœ… Send POST request
            response = requests.post(INSERT_URL, json=data, headers=HEADERS)
            response_json = response.json()

            # âœ… Debugging - Print server response
            print(f"ğŸ“œ Backend Response (Status {response.status_code}): {response_json}")

            if response.status_code == 201:
                print(f"âœ… Successfully sent {json_file}!")
            else:
                print(f"âŒ Failed to send {json_file} (Server Response): {response.text}")

        except json.JSONDecodeError:
            print(f"âŒ Error: Invalid JSON format in {json_file}. Skipping but NOT deleting it.")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Network error while sending {json_file}: {str(e)}")
            print("âš ï¸ Keeping the JSON file for retry.")
        except Exception as e:
            print(f"âŒ Unexpected error processing {json_file}: {str(e)}")

    print("âœ… All objectified JSONs have been processed.")


# âœ… Main Function
def main():
    news_dirs = find_matched_news_dirs(BASE_DIR)

    if not news_dirs:
        print("âŒ No `MatchedNewsData-*` directories found!")
        return

    print(f"ğŸ” Found {len(news_dirs)} directories to process.")

    # âœ… Delete old articles before inserting new ones
    print("ğŸ—‘ Deleting old articles before inserting new ones...")
    try:
        requests.get("http://127.0.0.1:8000/api/delete_articles/")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Error deleting old articles: {e}")

    for news_dir in news_dirs:
        print(f"ğŸ“‚ Processing: {news_dir}")
        articles = read_json_files(news_dir)

        if not articles:
            print(f"âš ï¸ No articles found in {news_dir}, skipping...")
            continue

        # âœ… Ensure no duplicate processing
        unique_titles = set()
        filtered_articles = []

        for article in articles:
            title = article.get("title", "").strip()
            if title and title not in unique_titles:
                unique_titles.add(title)
                filtered_articles.append(article)

        if not filtered_articles:
            print(f"âš ï¸ No new unique articles found in {news_dir}, skipping...")
            continue

        # âœ… AI Processing (COMMENT THIS IF NOT NEEDED)
        processed_data = process_articles_with_ai(filtered_articles)

        # âœ… Save locally
        save_json_output(processed_data)

        # Step 1: Send new JSON files **without deleting them unless uploaded successfully**
        send_json_files()

        print("âœ… Process completed successfully!")

    print("âœ… All directories processed successfully!")

# âœ… Run the script
if __name__ == "__main__":
    main()
